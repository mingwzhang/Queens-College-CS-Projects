# -*- coding: utf-8 -*-
"""Zhang_Tong_CS780_VOC2012_UNet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c_wmNUHOQcaqUMLNMkh1JYX5rftSMSEc
"""

!pip -q install tqdm opencv-python

import os, random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

import torchvision
import torchvision.transforms.functional as TF
from torchvision import models

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = True

set_seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

WORKERS = 2 if torch.cuda.is_available() else 0

URL = "https://data.brainchip.com/dataset-mirror/voc/VOCtrainval_11-May-2012.tar"
TAR_FILE = "VOCtrainval_11-May-2012.tar"
VOC_ROOT = "VOCdevkit/VOC2012"

marker = os.path.join(VOC_ROOT, "ImageSets", "Segmentation", "train.txt")

if not os.path.exists(marker):
    if not os.path.exists(TAR_FILE):
        print("Downloading VOC 2012 tar, resume enabled...")
        !wget -c "{URL}" -O "{TAR_FILE}"
    else:
        print("Tar already exists, extracting...")

    print("Extracting tar...")
    !tar -xf "{TAR_FILE}"
else:
    print("VOC 2012 already extracted at", VOC_ROOT)

CLASSES = ["background","person","car","dog","cat","motorbike","train","bus"]
NUM_CLASSES = 8
IGNORE_INDEX = 255

# VOC 2012 class ids, background=0, person=15, car=7, dog=12, cat=8, motorbike=14, train=19, bus=6
VOC_TO_8 = {
    0: 0,
    15: 1,
    7: 2,
    12: 3,
    8: 4,
    14: 5,
    19: 6,
    6: 7
}

IMG_SIZE = 256

IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD  = [0.229, 0.224, 0.225]

def normalize_tensor(x):
    return TF.normalize(x, IMAGENET_MEAN, IMAGENET_STD)

def denormalize_tensor(x):
    mean = torch.tensor(IMAGENET_MEAN, device=x.device).view(3,1,1)
    std  = torch.tensor(IMAGENET_STD, device=x.device).view(3,1,1)
    return (x * std + mean).clamp(0, 1)

def rand_resized_crop_params(H, W, scale=(0.8, 1.0), ratio=(0.9, 1.1), tries=10):
    area = H * W
    for _ in range(tries):
        target_area = random.uniform(*scale) * area
        aspect = random.uniform(*ratio)
        h = int(round((target_area * aspect) ** 0.5))
        w = int(round((target_area / aspect) ** 0.5))
        if 0 < h <= H and 0 < w <= W:
            i = random.randint(0, H - h)
            j = random.randint(0, W - w)
            return i, j, h, w
    s = min(H, W)
    i = (H - s) // 2
    j = (W - s) // 2
    return i, j, s, s

def joint_transform(img, mask, train: bool):
    if train:
        if random.random() < 0.5:
            img = TF.hflip(img)
            mask = TF.hflip(mask)

        H, W = img.size[1], img.size[0]
        i, j, h, w = rand_resized_crop_params(H, W)
        img = TF.resized_crop(img, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=Image.BILINEAR)
        mask = TF.resized_crop(mask, i, j, h, w, size=[IMG_SIZE, IMG_SIZE], interpolation=Image.NEAREST)

        img = TF.adjust_brightness(img, 1.0 + random.uniform(-0.15, 0.15))
        img = TF.adjust_contrast(img,   1.0 + random.uniform(-0.15, 0.15))
        img = TF.adjust_saturation(img, 1.0 + random.uniform(-0.10, 0.10))
    else:
        img = TF.resize(img, [IMG_SIZE, IMG_SIZE], interpolation=Image.BILINEAR)
        mask = TF.resize(mask, [IMG_SIZE, IMG_SIZE], interpolation=Image.NEAREST)

    return img, mask

class VOCDataset8(Dataset):
    def __init__(self, root, split="train", train_aug=False, filter_to_classes=True):
        self.root = root
        self.split = split
        self.train_aug = train_aug

        with open(os.path.join(root, "ImageSets/Segmentation", split + ".txt")) as f:
            ids = f.read().splitlines()

        self.img_dir = os.path.join(root, "JPEGImages")
        self.mask_dir = os.path.join(root, "SegmentationClass")

        if filter_to_classes:
            kept = []
            fg_ids = [k for k in VOC_TO_8.keys() if k != 0]
            for img_id in tqdm(ids, desc=f"Filtering {split}"):
                m = np.array(Image.open(os.path.join(self.mask_dir, img_id + ".png")))
                present = set(np.unique(m).tolist())
                if any((k in present) for k in fg_ids):
                    kept.append(img_id)
            self.ids = kept
        else:
            self.ids = ids

    def __len__(self):
        return len(self.ids)

    def encode_mask(self, mask_pil):
        m = np.array(mask_pil, dtype=np.uint8)
        out = np.full_like(m, fill_value=IGNORE_INDEX, dtype=np.uint8)

        out[m == 255] = IGNORE_INDEX  # VOC ignore
        for voc_id, mapped in VOC_TO_8.items():
            out[m == voc_id] = mapped

        return out

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img = Image.open(os.path.join(self.img_dir, img_id + ".jpg")).convert("RGB")
        mask = Image.open(os.path.join(self.mask_dir, img_id + ".png"))

        img, mask = joint_transform(img, mask, train=self.train_aug)
        mask = self.encode_mask(mask)

        img_t = TF.to_tensor(img)
        img_t = normalize_tensor(img_t)
        mask_t = torch.from_numpy(mask.astype(np.int64))

        return img_t, mask_t

train_ds = VOCDataset8(VOC_ROOT, split="train", train_aug=True,  filter_to_classes=True)
val_ds   = VOCDataset8(VOC_ROOT, split="val",   train_aug=False, filter_to_classes=True)

train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,  num_workers=WORKERS, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, num_workers=WORKERS, pin_memory=True)

print("Train samples:", len(train_ds))
print("Val samples:", len(val_ds))

def show_sample(img_t, mask_t, title=""):
    img = denormalize_tensor(img_t).permute(1,2,0).cpu().numpy()
    mask = mask_t.cpu().numpy()

    plt.figure(figsize=(10,3))
    plt.subplot(1,2,1)
    plt.title(f"{title} image")
    plt.imshow(img)
    plt.axis("off")

    plt.subplot(1,2,2)
    plt.title(f"{title} mask")
    plt.imshow(mask, cmap="tab10", vmin=0, vmax=NUM_CLASSES-1)
    plt.axis("off")
    plt.show()

img_b, mask_b = next(iter(train_loader))
show_sample(img_b[0], mask_b[0], title="train")

@torch.no_grad()
def compute_metrics(logits, mask):
    pred = logits.argmax(1)
    valid = (mask != IGNORE_INDEX)

    if valid.sum().item() == 0:
        return 0.0, [float("nan")]*NUM_CLASSES, float("nan")

    pixel_acc = (pred[valid] == mask[valid]).float().mean().item()

    ious = []
    for c in range(NUM_CLASSES):
        p = (pred == c) & valid
        g = (mask == c) & valid
        inter = (p & g).sum().item()
        union = (p | g).sum().item()
        ious.append(float("nan") if union == 0 else inter / union)

    miou = float(np.nanmean(ious))
    return pixel_acc, ious, miou

def print_metrics(pixel_acc, ious, miou, prefix=""):
    print(f"{prefix}Pixel Accuracy: {pixel_acc:.4f}")
    print(f"{prefix}Per-class IoU:")
    for name, v in zip(CLASSES, ious):
        if np.isnan(v):
            print(f"{prefix}  {name}: nan")
        else:
            print(f"{prefix}  {name}: {v:.3f}")
    print(f"{prefix}Mean IoU: {miou:.4f}")

def compute_class_weights(dataset, max_samples=200):
    counts = np.zeros(NUM_CLASSES, dtype=np.int64)

    n = min(len(dataset), max_samples)
    for i in tqdm(range(n), desc="Computing class weights"):
        _, m = dataset[i]
        m = m.numpy()
        valid = (m != IGNORE_INDEX)
        m = m[valid]
        if m.size == 0:
            continue
        for c in range(NUM_CLASSES):
            counts[c] += np.sum(m == c)

    counts = np.maximum(counts, 1)
    freq = counts / counts.sum()
    w = 1.0 / np.log(1.02 + freq)
    w = w / w.mean()
    return torch.tensor(w, dtype=torch.float32), counts

class_weights, class_counts = compute_class_weights(train_ds, max_samples=200)
print("Approx pixel counts (sampled):", class_counts)
print("Class weights:", class_weights.numpy())

class_weights = class_weights.to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=IGNORE_INDEX)

class ConvBlock(nn.Module):
    def __init__(self, in_c, out_c):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_c),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_c, out_c, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_c),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        return self.net(x)

class UNetBaseline(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES, base=64):
        super().__init__()
        self.enc1 = ConvBlock(3, base)
        self.enc2 = ConvBlock(base, base*2)
        self.enc3 = ConvBlock(base*2, base*4)
        self.enc4 = ConvBlock(base*4, base*8)

        self.pool = nn.MaxPool2d(2)

        self.center = ConvBlock(base*8, base*16)

        self.up3 = ConvBlock(base*16 + base*8, base*8)
        self.up2 = ConvBlock(base*8  + base*4, base*4)
        self.up1 = ConvBlock(base*4  + base*2, base*2)
        self.up0 = ConvBlock(base*2  + base,   base)

        self.head = nn.Conv2d(base, num_classes, 1)

    def forward(self, x):
        e1 = self.enc1(x)          # H
        e2 = self.enc2(self.pool(e1))  # H/2
        e3 = self.enc3(self.pool(e2))  # H/4
        e4 = self.enc4(self.pool(e3))  # H/8

        c = self.center(self.pool(e4)) # H/16

        d3 = F.interpolate(c, size=e4.shape[-2:], mode="bilinear", align_corners=False)
        d3 = self.up3(torch.cat([d3, e4], dim=1))

        d2 = F.interpolate(d3, size=e3.shape[-2:], mode="bilinear", align_corners=False)
        d2 = self.up2(torch.cat([d2, e3], dim=1))

        d1 = F.interpolate(d2, size=e2.shape[-2:], mode="bilinear", align_corners=False)
        d1 = self.up1(torch.cat([d1, e2], dim=1))

        d0 = F.interpolate(d1, size=e1.shape[-2:], mode="bilinear", align_corners=False)
        d0 = self.up0(torch.cat([d0, e1], dim=1))

        return self.head(d0)

class UpBlock(nn.Module):
    def __init__(self, in_c, skip_c, out_c):
        super().__init__()
        self.conv = ConvBlock(in_c + skip_c, out_c)

    def forward(self, x, skip):
        x = F.interpolate(x, size=skip.shape[-2:], mode="bilinear", align_corners=False)
        x = torch.cat([x, skip], dim=1)
        return self.conv(x)

class ResNet50UNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super().__init__()
        enc = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

        self.stem = nn.Sequential(enc.conv1, enc.bn1, enc.relu)  # 64, H/2
        self.pool = enc.maxpool                                  # H/4

        self.enc1 = enc.layer1  # 256, H/4
        self.enc2 = enc.layer2  # 512, H/8
        self.enc3 = enc.layer3  # 1024, H/16
        self.enc4 = enc.layer4  # 2048, H/32

        self.center = ConvBlock(2048, 1024)

        self.up3 = UpBlock(1024, 1024, 512)
        self.up2 = UpBlock(512,  512,  256)
        self.up1 = UpBlock(256,  256,  128)
        self.up0 = UpBlock(128,   64,   64)

        self.head = nn.Conv2d(64, num_classes, 1)

    def forward(self, x):
        x0 = self.stem(x)      # 64, H/2
        x1 = self.pool(x0)     # H/4
        x1 = self.enc1(x1)     # 256, H/4
        x2 = self.enc2(x1)     # 512, H/8
        x3 = self.enc3(x2)     # 1024, H/16
        x4 = self.enc4(x3)     # 2048, H/32

        c  = self.center(x4)   # 1024, H/32

        d3 = self.up3(c,  x3)  # 512,  H/16
        d2 = self.up2(d3, x2)  # 256,  H/8
        d1 = self.up1(d2, x1)  # 128,  H/4
        d0 = self.up0(d1, x0)  # 64,   H/2

        d0 = F.interpolate(d0, scale_factor=2, mode="bilinear", align_corners=False)  # H
        return self.head(d0)

def train_model(name, model, train_loader, val_loader, epochs=10, lr=2e-4):
    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

    history = {
        "train_loss": [],
        "val_pixel_acc": [],
        "val_miou": []
    }

    for epoch in range(1, epochs + 1):
        model.train()
        total = 0.0

        for img, mask in tqdm(train_loader, desc=f"{name} epoch {epoch}/{epochs}"):
            img = img.to(device, non_blocking=True)
            mask = mask.to(device, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                logits = model(img)
                loss = criterion(logits, mask)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total += loss.item()

        avg_loss = total / len(train_loader)
        history["train_loss"].append(avg_loss)

        model.eval()
        accs, mious = [], []
        for img, mask in val_loader:
            img = img.to(device, non_blocking=True)
            mask = mask.to(device, non_blocking=True)
            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                logits = model(img)
            acc, _, miou = compute_metrics(logits, mask)
            accs.append(acc)
            mious.append(miou)

        val_acc = float(np.mean(accs))
        val_miou = float(np.nanmean(mious))
        history["val_pixel_acc"].append(val_acc)
        history["val_miou"].append(val_miou)

        print(f"{name} epoch {epoch}, train loss {avg_loss:.4f}, val pixel acc {val_acc:.4f}, val mIoU {val_miou:.4f}")

    return model, history

EPOCHS_BASELINE = 30

baseline = UNetBaseline(NUM_CLASSES, base=64)
baseline, baseline_hist = train_model(
    name="baseline_unet",
    model=baseline,
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=EPOCHS_BASELINE,
    lr=2e-4
)

EPOCHS_RESNET = 30

resnet_unet = ResNet50UNet(NUM_CLASSES)
resnet_unet, resnet_hist = train_model(
    name="resnet50_unet",
    model=resnet_unet,
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=EPOCHS_RESNET,
    lr=2e-4
)

plt.figure(figsize=(6,4))
plt.plot(baseline_hist["train_loss"], marker="o")
plt.plot(resnet_hist["train_loss"], marker="o")
plt.title("Train Loss, baseline vs ResNet-50 U-Net")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.legend(["baseline_unet", "resnet50_unet"])
plt.show()

plt.figure(figsize=(6,4))
plt.plot(baseline_hist["val_miou"], marker="o")
plt.plot(resnet_hist["val_miou"], marker="o")
plt.title("Validation Mean IoU, baseline vs ResNet-50 U-Net")
plt.xlabel("Epoch")
plt.ylabel("mIoU")
plt.grid(True)
plt.legend(["baseline_unet", "resnet50_unet"])
plt.show()

@torch.no_grad()
def eval_full(model, loader, max_batches=None):
    model.eval()
    accs = []
    iou_lists = []
    mious = []

    for b, (img, mask) in enumerate(loader):
        if max_batches is not None and b >= max_batches:
            break
        img = img.to(device, non_blocking=True)
        mask = mask.to(device, non_blocking=True)
        logits = model(img)
        acc, ious, miou = compute_metrics(logits, mask)
        accs.append(acc)
        iou_lists.append(ious)
        mious.append(miou)

    mean_acc = float(np.mean(accs))
    mean_iou_per_class = list(np.nanmean(np.array(iou_lists), axis=0))
    mean_miou = float(np.nanmean(mious))
    return mean_acc, mean_iou_per_class, mean_miou

base_acc, base_ious, base_miou = eval_full(baseline, val_loader)
res_acc,  res_ious,  res_miou  = eval_full(resnet_unet, val_loader)

print("BASELINE RESULTS")
print_metrics(base_acc, base_ious, base_miou, prefix="  ")

print("\nRESNET-50 U-NET RESULTS")
print_metrics(res_acc, res_ious, res_miou, prefix="  ")

print("\nSUMMARY")
print(f"  baseline mIoU: {base_miou:.4f}, resnet50 mIoU: {res_miou:.4f}")
print(f"  baseline pixel acc: {base_acc:.4f}, resnet50 pixel acc: {res_acc:.4f}")

@torch.no_grad()
def visualize_compare(baseline_model, improved_model, loader, n=8):
    baseline_model.eval()
    improved_model.eval()

    img_b, mask_b = next(iter(loader))
    n = min(n, img_b.shape[0])

    img_b = img_b[:n].to(device)
    mask_b = mask_b[:n].to(device)

    logits_base = baseline_model(img_b)
    logits_imp  = improved_model(img_b)

    pred_base = logits_base.argmax(1).detach().cpu().numpy()
    pred_imp  = logits_imp.argmax(1).detach().cpu().numpy()
    gt        = mask_b.detach().cpu().numpy()
    imgs      = denormalize_tensor(img_b).detach().cpu().numpy()

    fig, axes = plt.subplots(n, 4, figsize=(16, 3*n))
    if n == 1:
        axes = np.expand_dims(axes, axis=0)

    for i in range(n):
        im = np.transpose(imgs[i], (1,2,0))

        axes[i,0].imshow(im)
        axes[i,0].set_title("Image")
        axes[i,0].axis("off")

        axes[i,1].imshow(gt[i], cmap="tab10", vmin=0, vmax=NUM_CLASSES-1)
        axes[i,1].set_title("Ground Truth")
        axes[i,1].axis("off")

        axes[i,2].imshow(pred_base[i], cmap="tab10", vmin=0, vmax=NUM_CLASSES-1)
        axes[i,2].set_title("Baseline U-Net")
        axes[i,2].axis("off")

        axes[i,3].imshow(pred_imp[i], cmap="tab10", vmin=0, vmax=NUM_CLASSES-1)
        axes[i,3].set_title("ResNet-50 U-Net")
        axes[i,3].axis("off")

    plt.tight_layout()
    plt.show()

visualize_compare(baseline, resnet_unet, val_loader, n=8)